{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "\n",
    "#initialize to use for later\n",
    "wine_train_set=0\n",
    "wine_test_set=0\n",
    "#code is the same as from over on the initial exploration notebook.\n",
    "dataframe = pd.read_csv(\"winequality-white.csv\")\n",
    "dataframeCopy = dataframe.copy()\n",
    "dataframeCopy.dropna(axis='index', how='any', inplace=True)\n",
    "dataframeCopy.dropna(axis='columns', how='any', inplace=True)\n",
    "\n",
    "#stratifying data on quality to predict it\n",
    "#reverted the test size\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=103)\n",
    "\n",
    "for train_indices, test_indices in splitter.split(dataframeCopy, dataframeCopy[\"quality\"]):\n",
    "    wine_train_set = dataframeCopy.iloc[train_indices]\n",
    "    wine_test_set = dataframeCopy.iloc[test_indices]\n",
    "    \n",
    "\n",
    "workingset=wine_train_set.copy()\n",
    "testingset=wine_test_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initial set of features for X and the target feature y.\n",
    "\n",
    "- My initial set of features for X are: residual sugar, alcohol, density,chlorides,volatile acidity, fixed acidity, sulphates, pH\n",
    " - The target feature for y is: quality \n",
    "\n",
    "- I chose these because in milestone one part b using these features I achieved an r-squared value of 0.2769342263179473, and a root mean squared error of: 0.7529132148619554 which were relatively close to the values of the training set, and overall for my dataset these values achieved the highest r-squared values, and root mean squared error values for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  16    0    0    0    0    0    0]\n",
      " [   0  130    0    0    0    0    0]\n",
      " [   0    0 1166    0    0    0    0]\n",
      " [   0    0    0 1758    0    0    0]\n",
      " [   0    0    0    0  704    0    0]\n",
      " [   0    0    0    0    0  140    0]\n",
      " [   0    0    0    0    0    0    4]]\n",
      "Accuracy is  1.0\n",
      "Precision is  1.0\n",
      "Sensitivity is  1.0\n",
      "F1 is  1.0\n",
      "Cross validation accuracies are:  [0.576530612244898, 0.5471938775510204, 0.5625, 0.5530012771392082, 0.5606641123882503]\n",
      "Cross validation f1 scores  are:  [0.5793953210504539, 0.5479319719338513, 0.5618041410237935, 0.5570180226934941, 0.5606869706999108]\n"
     ]
    }
   ],
   "source": [
    "#DECISION TREE FOR PART D-3 HERE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "y = workingset[\"quality\"]\n",
    "x = workingset[[\"residual sugar\", \"alcohol\", \"density\",\"chlorides\",\"volatile acidity\", \"fixed acidity\", \"sulphates\", \"pH\"]]\n",
    "\n",
    "tree_classifier = DecisionTreeClassifier()\n",
    "tree_classifier.fit(x,y)\n",
    "\n",
    "y_predicted = tree_classifier.predict(x)\n",
    "matrix = confusion_matrix(y, y_predicted)\n",
    "print(matrix)\n",
    "print (\"Accuracy is \", accuracy_score(y, y_predicted))\n",
    "\n",
    "# We have to specify how to combine for the multiclassifications\n",
    "print (\"Precision is \", precision_score(y, y_predicted, average=\"weighted\"))\n",
    "print (\"Sensitivity is \", recall_score(y, y_predicted, average=\"weighted\"))\n",
    "print (\"F1 is \", f1_score(y, y_predicted, average=\"weighted\"))\n",
    "\n",
    "#results are likely overfitting so the below checks it.\n",
    "# computed these metrics with the kFolds code from the lecutre material specifically because all of my confusion matrices were perfect and so was my accuracy and F-score.\n",
    "validation_accuracy = []\n",
    "validation_f1 =[]\n",
    "fold_and_validate = KFold(n_splits=5, shuffle=True, random_state=145)\n",
    "for train_set_indices, validation_set_indices in fold_and_validate.split(x):\n",
    "    cv_train_set = x.iloc[train_set_indices]\n",
    "    cv_train_target = y.iloc[train_set_indices]\n",
    "    #print(cv_train_set)\n",
    "    \n",
    "    cv_decision_tree = DecisionTreeClassifier()\n",
    "    cv_decision_tree.fit(cv_train_set, cv_train_target)\n",
    "    \n",
    "    cv_xvalidation = x.iloc[validation_set_indices]\n",
    "    cv_y_true = y.iloc[validation_set_indices]\n",
    "    cv_y_predicted = cv_decision_tree.predict(cv_xvalidation)\n",
    "    \n",
    "    cv_accuracy_score = accuracy_score(cv_y_true, cv_y_predicted)\n",
    "    cv_f1_score = f1_score(cv_y_true, cv_y_predicted,  average=\"weighted\")\n",
    "    validation_accuracy.append(cv_accuracy_score)\n",
    "    validation_f1.append(cv_f1_score)\n",
    "    \n",
    "print(\"Cross validation accuracies are: \", validation_accuracy)\n",
    "print(\"Cross validation f1 scores  are: \", validation_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4-Thoughts on results\n",
    "\n",
    "- Having perfect score for accuracy and other metrics with 1.0 would be great if it were not for the following: \n",
    "- It seems like to me that the data is likely overfitting because the confusion matrix is perfect with only one value per node for every node in this case so it doesn't seem like it's predicting anything.\n",
    "- also per my in-class notes I see I am using more features than in the examples so I suspect that the amount of parameters I have is contributing to an overfit.\n",
    "- To add to the first point about not predicting anything: the validation accuracies are fairly consistent and so too are the f1 scores but they differ greatly from the overall dataset's scores indicating that the model wouldn't do as great on unseen data, where as it's doing perfectly on the known data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    7    9    0    0    0]\n",
      " [   0    0   74   56    0    0    0]\n",
      " [   0    0  608  558    0    0    0]\n",
      " [   0    0  312 1446    0    0    0]\n",
      " [   0    0   21  683    0    0    0]\n",
      " [   0    0    2  138    0    0    0]\n",
      " [   0    0    0    4    0    0    0]]\n",
      "Accuracy is  0.5242470648289944\n",
      "Precision is  0.40089459833638663\n",
      "Sensitivity is  0.5242470648289944\n",
      "F1 is  0.4441846223575155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s529027\\Documents\\GitHub\\ml-s24-project-DerekVolner\\ds-venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation accuracies are:  [0.4885204081632653, 0.5025510204081632, 0.4897959183673469, 0.5210727969348659, 0.46871008939974457]\n",
      "Cross validation f1 scores  are:  [0.4183026794633937, 0.42580947890174303, 0.42172918484473465, 0.4434337947004362, 0.380692231974421]\n"
     ]
    }
   ],
   "source": [
    "#seeing if I can do better:  using SVM or another multi-classifier\n",
    "#Linear Support Vector Classification using RBF\n",
    "from sklearn.svm import SVC \n",
    "y = workingset[\"quality\"]\n",
    "x = workingset[[\"residual sugar\", \"alcohol\", \"density\",\"chlorides\",\"volatile acidity\", \"fixed acidity\", \"sulphates\", \"pH\"]]\n",
    "\n",
    "#I tested  rbf, linear, poly, and sigmoid, linear had the best results out of the 4.\n",
    "svm_classifier_func = SVC(kernel = 'linear')\n",
    "svm_classifier_func.fit(x,y)\n",
    "\n",
    "\n",
    "y_predicted = svm_classifier_func.predict(x)\n",
    "matrix = confusion_matrix(y, y_predicted)\n",
    "print(matrix)\n",
    "print (\"Accuracy is \", accuracy_score(y, y_predicted))\n",
    "print (\"Precision is \", precision_score(y, y_predicted, average=\"weighted\"))\n",
    "print (\"Sensitivity is \", recall_score(y, y_predicted, average=\"weighted\"))\n",
    "print (\"F1 is \", f1_score(y, y_predicted, average=\"weighted\"))\n",
    "\n",
    "\n",
    "\n",
    "#use K-fold to check for overfitting potentially\n",
    "validation_accuracy = []\n",
    "validation_f1 =[]\n",
    "fold_and_validate = KFold(n_splits=5, shuffle=True, random_state=145)\n",
    "for train_set_indices, validation_set_indices in fold_and_validate.split(x):\n",
    "    cv_train_set = x.iloc[train_set_indices]\n",
    "    cv_train_target = y.iloc[train_set_indices]\n",
    "    \n",
    "    cv_svc = SVC()\n",
    "    cv_svc.fit(cv_train_set, cv_train_target)\n",
    "    \n",
    "    cv_xvalidation = x.iloc[validation_set_indices]\n",
    "    cv_y_true = y.iloc[validation_set_indices]\n",
    "    cv_y_predicted = cv_svc.predict(cv_xvalidation)\n",
    "    \n",
    "    cv_accuracy_score = accuracy_score(cv_y_true, cv_y_predicted)\n",
    "    cv_f1_score = f1_score(cv_y_true, cv_y_predicted,  average=\"weighted\")\n",
    "    validation_accuracy.append(cv_accuracy_score)\n",
    "    validation_f1.append(cv_f1_score)\n",
    "    \n",
    "print(\"Cross validation accuracies are: \", validation_accuracy)\n",
    "print(\"Cross validation f1 scores  are: \", validation_f1)\n",
    "\n",
    "#the svm classifier with the linear seems to be actually predicting now even if it is misclassifying not sure but this seems like an improvement over a model that simply\n",
    "#does not do any prediction so I am submitting this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   3   1   0   0   0]\n",
      " [  0   0  20  13   0   0   0]\n",
      " [  0   0 163 128   0   0   0]\n",
      " [  0   0  99 341   0   0   0]\n",
      " [  0   0  13 163   0   0   0]\n",
      " [  0   0   4  31   0   0   0]\n",
      " [  0   0   0   1   0   0   0]]\n",
      "Accuracy is  0.5142857142857142\n",
      "Precision is  0.3860824989903363\n",
      "Sensitivity is  0.5142857142857142\n",
      "F1 is  0.43712679839754853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s529027\\Documents\\GitHub\\ml-s24-project-DerekVolner\\ds-venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation accuracies are:  [0.4387755102040816, 0.47959183673469385, 0.42346938775510207, 0.413265306122449, 0.4489795918367347]\n",
      "Cross validation f1 scores  are:  [0.3517312362297164, 0.32050949299580456, 0.2519566966571575, 0.28697604596689663, 0.36596623185356614]\n"
     ]
    }
   ],
   "source": [
    "#FINAL EVALUATION WTIH TEST SET\n",
    "from sklearn.svm import SVC \n",
    "y = testingset[\"quality\"]\n",
    "x = testingset[[\"residual sugar\", \"alcohol\", \"density\",\"chlorides\",\"volatile acidity\", \"fixed acidity\", \"sulphates\", \"pH\"]]\n",
    "\n",
    "\n",
    "svm_classifier_func = SVC(kernel = 'linear')\n",
    "svm_classifier_func.fit(x,y)\n",
    "\n",
    "\n",
    "y_predicted = svm_classifier_func.predict(x)\n",
    "matrix = confusion_matrix(y, y_predicted)\n",
    "print(matrix)\n",
    "print (\"Accuracy is \", accuracy_score(y, y_predicted))\n",
    "print (\"Precision is \", precision_score(y, y_predicted, average=\"weighted\"))\n",
    "print (\"Sensitivity is \", recall_score(y, y_predicted, average=\"weighted\"))\n",
    "print (\"F1 is \", f1_score(y, y_predicted, average=\"weighted\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "validation_accuracy = []\n",
    "validation_f1 =[]\n",
    "fold_and_validate = KFold(n_splits=5, shuffle=True, random_state=145)\n",
    "for train_set_indices, validation_set_indices in fold_and_validate.split(x):\n",
    "    cv_train_set = x.iloc[train_set_indices]\n",
    "    cv_train_target = y.iloc[train_set_indices]\n",
    "    \n",
    "    cv_svc = SVC()\n",
    "    cv_svc.fit(cv_train_set, cv_train_target)\n",
    "    \n",
    "    cv_xvalidation = x.iloc[validation_set_indices]\n",
    "    cv_y_true = y.iloc[validation_set_indices]\n",
    "    cv_y_predicted = cv_svc.predict(cv_xvalidation)\n",
    "    \n",
    "    cv_accuracy_score = accuracy_score(cv_y_true, cv_y_predicted)\n",
    "    cv_f1_score = f1_score(cv_y_true, cv_y_predicted,  average=\"weighted\")\n",
    "    validation_accuracy.append(cv_accuracy_score)\n",
    "    validation_f1.append(cv_f1_score)\n",
    "    \n",
    "print(\"Cross validation accuracies are: \", validation_accuracy)\n",
    "print(\"Cross validation f1 scores  are: \", validation_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the parameters you found and discuss what you have learned. \n",
    "# Part D-7\n",
    "- I have learned that my model is inaccurate, and misidentifies features but that by using svm linear we do achieve predictive results which is better then the decision tree's result which was vastly overfitting the model.\n",
    "- Also, it seems to me from both the test set and training set using svm 'linear' that the indication of low F1 and Accuracy scores means that the model does not work well with predicting new and unknown data values accurately to the set which aligns with my findings from the linear regression notebook where the highest r2 was 0.29 indicating a low predictive power which in the case of actually using it to predict something means it will not do well, which it did not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
