{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "\n",
    "#initialize to use for later\n",
    "wine_train_set=0\n",
    "wine_test_set=0\n",
    "#code is the same as from over on the initial exploration notebook.\n",
    "dataframe = pd.read_csv(\"winequality-white.csv\")\n",
    "dataframeCopy = dataframe.copy()\n",
    "dataframeCopy.dropna(axis='index', how='any', inplace=True)\n",
    "dataframeCopy.dropna(axis='columns', how='any', inplace=True)\n",
    "\n",
    "#stratifying data on quality to predict it\n",
    "#reverted the test size\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=103)\n",
    "\n",
    "for train_indices, test_indices in splitter.split(dataframeCopy, dataframeCopy[\"quality\"]):\n",
    "    wine_train_set = dataframeCopy.iloc[train_indices]\n",
    "    wine_test_set = dataframeCopy.iloc[test_indices]\n",
    "    \n",
    "\n",
    "workingset=wine_train_set.copy()\n",
    "testingset=wine_test_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initial set of features for X and the target feature y.\n",
    "\n",
    "- My initial set of features for X are: residual sugar, alcohol, density,chlorides,volatile acidity, fixed acidity, sulphates, pH\n",
    " - The target feature for y is: quality \n",
    "\n",
    "- I chose these because in milestone one part b using these features I achieved an r-squared value of 0.2769342263179473, and a root mean squared error of: 0.7529132148619554 which were relatively close to the values of the training set, and overall for my dataset these values achieved the highest r-squared values, and root mean squared error values for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  16    0    0    0    0    0    0]\n",
      " [   0  130    0    0    0    0    0]\n",
      " [   0    0 1166    0    0    0    0]\n",
      " [   0    0    0 1758    0    0    0]\n",
      " [   0    0    0    0  704    0    0]\n",
      " [   0    0    0    0    0  140    0]\n",
      " [   0    0    0    0    0    0    4]]\n",
      "Accuracy is  1.0\n",
      "Precision is  1.0\n",
      "Sensitivity is  1.0\n",
      "F1 is  1.0\n",
      "Cross validation accuracies are:  [0.576530612244898, 0.5471938775510204, 0.5625, 0.5530012771392082, 0.5606641123882503]\n",
      "Cross validation f1 scores  are:  [0.5793953210504539, 0.5479319719338513, 0.5618041410237935, 0.5570180226934941, 0.5606869706999108]\n"
     ]
    }
   ],
   "source": [
    "#DECISION TREE FOR PART D-3 HERE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "y = workingset[\"quality\"]\n",
    "x = workingset[[\"residual sugar\", \"alcohol\", \"density\",\"chlorides\",\"volatile acidity\", \"fixed acidity\", \"sulphates\", \"pH\"]]\n",
    "\n",
    "tree_classifier = DecisionTreeClassifier()\n",
    "tree_classifier.fit(x,y)\n",
    "\n",
    "y_predicted = tree_classifier.predict(x)\n",
    "matrix = confusion_matrix(y, y_predicted)\n",
    "print(matrix)\n",
    "print (\"Accuracy is \", accuracy_score(y, y_predicted))\n",
    "\n",
    "# We have to specify how to combine for the multiclassifications\n",
    "print (\"Precision is \", precision_score(y, y_predicted, average=\"weighted\"))\n",
    "print (\"Sensitivity is \", recall_score(y, y_predicted, average=\"weighted\"))\n",
    "print (\"F1 is \", f1_score(y, y_predicted, average=\"weighted\"))\n",
    "\n",
    "#results are likely overfitting so the below checks it.\n",
    "# computed these metrics with the kFolds code from the lecutre material specifically because all of my confusion matrices were perfect and so was my accuracy and F-score.\n",
    "validation_accuracy = []\n",
    "validation_f1 =[]\n",
    "fold_and_validate = KFold(n_splits=5, shuffle=True, random_state=145)\n",
    "for train_set_indices, validation_set_indices in fold_and_validate.split(x):\n",
    "    cv_train_set = x.iloc[train_set_indices]\n",
    "    cv_train_target = y.iloc[train_set_indices]\n",
    "    #print(cv_train_set)\n",
    "    \n",
    "    cv_decision_tree = DecisionTreeClassifier()\n",
    "    cv_decision_tree.fit(cv_train_set, cv_train_target)\n",
    "    \n",
    "    cv_xvalidation = x.iloc[validation_set_indices]\n",
    "    cv_y_true = y.iloc[validation_set_indices]\n",
    "    cv_y_predicted = cv_decision_tree.predict(cv_xvalidation)\n",
    "    \n",
    "    cv_accuracy_score = accuracy_score(cv_y_true, cv_y_predicted)\n",
    "    cv_f1_score = f1_score(cv_y_true, cv_y_predicted,  average=\"weighted\")\n",
    "    validation_accuracy.append(cv_accuracy_score)\n",
    "    validation_f1.append(cv_f1_score)\n",
    "    \n",
    "print(\"Cross validation accuracies are: \", validation_accuracy)\n",
    "print(\"Cross validation f1 scores  are: \", validation_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4-Thoughts on results\n",
    "\n",
    "- Having perfect score for accuracy and other metrics with 1.0 would be great if it were not for the following: \n",
    "- It seems like to me that the data is likely overfitting because the confusion matrix is perfect with only one value per node for every node in this case so it doesn't seem like it's predicting anything.\n",
    "- also per my in-class notes I see I am using more features than in the examples so I suspect that the amount of parameters I have is contributing to an overfit.\n",
    "- To add to the first point about not predicting anything: the validation accuracies are fairly consistent and so too are the f1 scores but they differ greatly from the overall dataset's scores indicating that the model wouldn't do as great on unseen data, where as it's doing perfectly on the known data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
